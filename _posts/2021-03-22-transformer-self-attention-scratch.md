---
title: "Self-Attention & Transformers from Scratch"
categories:
  - Machine-Learning
  - Competence
tags:
  - transformer
  - pytorch
  - notebook
toc: true
---

Have you ever heard words like "BERT" or "GPT-2/3"? Perhaps you've even seen the amazing results by GPT?
These Neural Networks (yes, that's what they're) are at the frontier of Natural Language Processing (NLP) and are called "Transformers" because they all share (almost) the same architecture.
What's interesting about Transformers is that they keep on creating State-of-the-Art results time and time again because of the huge scalability.


Unfortunately the Afry South blog don't support notebooks, but feel free to read or even run the notebook through the link ðŸ‘‡.  

[blog.londogard.com](https://blog.londogard.com/nlp/deep-learning/2021/02/18/transformers-explained.html).
